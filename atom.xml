<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://baiheworld.github.io</id>
    <title>白鹤的世界</title>
    <updated>2025-02-08T02:28:34.253Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://baiheworld.github.io"/>
    <link rel="self" href="https://baiheworld.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://baiheworld.github.io/images/avatar.png</logo>
    <icon>https://baiheworld.github.io/favicon.ico</icon>
    <rights>All rights reserved 2025, 白鹤的世界</rights>
    <entry>
        <title type="html"><![CDATA[在WSL2环境下使用Ollama本地部署DeepSeek-R1-14B模型]]></title>
        <id>https://baiheworld.github.io/post/zai-wsl2-huan-jing-xia-shi-yong-ollama-ben-di-bu-shu-deepseek-r1-14b-mo-xing/</id>
        <link href="https://baiheworld.github.io/post/zai-wsl2-huan-jing-xia-shi-yong-ollama-ben-di-bu-shu-deepseek-r1-14b-mo-xing/">
        </link>
        <updated>2025-02-08T01:49:29.000Z</updated>
        <content type="html"><![CDATA[<p>由于近期deepseek在全球范围内的爆火，我想正常的体验deepseek功能都变的很困难。于是萌生了本地部署DeepSeek-R1模型的想法，虽然知道本地部署的版本和官网的版本体验上存在着巨大的差距，但依然对DeepSeek-R1:14B模型的能力很好奇。</p>
<blockquote>
<p>一年前我本地部署过llama-13b模型，实际体验并不好</p>
</blockquote>
<hr>
<p>在本篇文章中，我将详细介绍如何在 <strong>Windows 11</strong> 系统下，利用 <strong>WSL2</strong> 环境和 <strong>NVIDIA 4060 Ti</strong>  16GB显卡，本地部署 <strong>DeepSeek-R1-14B</strong> 模型，并通过 <strong>Ollama</strong> 提供服务。本文将涵盖硬件配置、环境搭建、模型部署以及性能优化的完整流程。</p>
<h2 id="硬件与软件配置">硬件与软件配置</h2>
<h3 id="硬件配置">硬件配置：</h3>
<ul>
<li><strong>处理器</strong>：AMD Ryzen 5 5700X</li>
<li><strong>显卡</strong>：NVIDIA GeForce RTX 4060 Ti（16GB 显存版本）</li>
<li><strong>内存</strong>：32GB DDR4</li>
<li><strong>存储</strong>：充足的空间用于存放模型和数据</li>
</ul>
<h3 id="软件配置">软件配置：</h3>
<ul>
<li><strong>操作系统</strong>：Windows 11 专业工作站版 24H2</li>
<li><strong>WSL2 系统</strong>：Ubuntu 20.04</li>
<li><strong>NVIDIA 驱动</strong>：560.94</li>
<li><strong>CUDA Toolkit版本</strong>： 12.6</li>
</ul>
<hr>
<h2 id="安装与配置-wsl2-环境">安装与配置 WSL2 环境</h2>
<h3 id="1-安装-windows-subsystem-for-linux-wsl2">1. 安装 Windows Subsystem for Linux (WSL2)</h3>
<p>如果尚未安装 WSL2，请参考以下步骤操作：</p>
<ol>
<li>在“启用或关闭 Windows 功能”中分别勾选<strong>Hyper-V</strong>和<strong>适用于Linux的Windows子系统</strong>这两个选项，点击确定后自动安装所需的系统组件。<br>
<img src="https://baiheworld.github.io/post-images/1738981511011.png" alt="" loading="lazy"><br>
<img src="https://baiheworld.github.io/post-images/1738981545760.png" alt="" loading="lazy"></li>
<li>在PowerShell中执行“wsl --install”</li>
<li>在微软应用商店中搜索“ubuntu”，找到需要的版本，点击安装（图中是已经安装完成了）<br>
<img src="https://baiheworld.github.io/post-images/1738981569360.png" alt="" loading="lazy"></li>
<li>最后检查是否使用的是wsl2版本</li>
</ol>
<pre><code class="language-sh">C:\Users\user&gt;wsl -l -v
  NAME            STATE           VERSION
* Ubuntu-20.04    Running         2
</code></pre>
<h3 id="2-配置-nvidia-显卡驱动和-cuda">2. 配置 NVIDIA 显卡驱动和 CUDA</h3>
<ol>
<li>首先打开CUDA和windows驱动的<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">版本对照表</a> ，<br>
<code>可以看到我的驱动版本最高可以安装CUDA 12.6 Update 2版本，这里我选择是CUDA 12.6 GA 版本</code><br>
<img src="https://baiheworld.github.io/post-images/1738981596363.png" alt="" loading="lazy"></li>
<li>安装 CUDA Toolkit：
<ul>
<li>下载 CUDA 安装程序：<a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive</a></li>
<li>按照提示完成安装，确保选择与您的 GPU 和操作系统兼容的版本（建议选择 CUDA 12.x）。<br>
<img src="https://baiheworld.github.io/post-images/1738981604848.png" alt="" loading="lazy"></li>
</ul>
</li>
</ol>
<hr>
<h2 id="安装与配置-ollama">安装与配置 Ollama</h2>
<h3 id="1-下载并安装-ollama">1. 下载并安装 Ollama</h3>
<p>Ollama 是一个用于部署和运行大语言模型的工具，这里直接使用官方脚本一键安装Ollama：</p>
<pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
<h3 id="2-配置-ollama-环境">2. 配置 Ollama 环境</h3>
<ul>
<li>启动 Ollama 服务：<pre><code class="language-bash">./ollama serve 
</code></pre>
</li>
<li>验证 Ollama 是否正常运行：<br>
打开浏览器访问 <code>http://localhost:11434</code>，如果看到欢迎界面，则表示 Ollama 已成功启动。</li>
</ul>
<hr>
<h2 id="下载与部署-deepseek-r1-14b-模型">下载与部署 DeepSeek-R1-14B 模型</h2>
<h3 id="1-下载模型">1. 下载模型</h3>
<pre><code class="language-bash">ollama pull deepseek-r1:14b
</code></pre>
<h3 id="2-启动模型服务">2. 启动模型服务</h3>
<p>运行以下命令启动 DeepSeek-R1-14B 模型：</p>
<pre><code class="language-bash">ollama run deepseek-r1:14b
</code></pre>
<hr>
<h2 id="验证与测试">验证与测试</h2>
<h3 id="1-使用-curl-测试-api">1. 使用 curl 测试 API</h3>
<p>打开一个新的终端，运行以下命令发送请求：</p>
<pre><code class="language-bash">curl -X POST &quot;http://localhost:11434/api/generate&quot; \
     -H &quot;Content-Type: application/json&quot; \
     -d '{&quot;model&quot;: &quot;deepseek-r1:14b&quot;, &quot;prompt&quot;: &quot;hi，你是谁？&quot;, &quot;stream&quot;: false}'
</code></pre>
<h3 id="2-验证模型响应">2. 验证模型响应</h3>
<p>如果一切正常，您将收到类似以下的响应：</p>
<pre><code class="language-json">{
  &quot;model&quot;: &quot;deepseek-r1:14b&quot;,
  &quot;created_at&quot;: &quot;2025-02-06T09:31:47.730845582Z&quot;,
  &quot;response&quot;: &quot;&lt;think&gt;\n\n&lt;/think&gt;\n\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。&quot;,
  &quot;done&quot;: true,
  &quot;done_reason&quot;: &quot;stop&quot;,
  &quot;context&quot;: [
    151644,6023,3837,105043,100165,11319,151645,151648,271,151649,271,
111308,6313,104198,67071,105538,102217,30918,50984,9909,33464,39350,7552,
73218,100013,9370,100168,110498,33464,39350,10911,16,1773,29524,87026,
110117,99885,86119,3837,105351,99739,35946,111079,113445,100364,1773
  ],
  &quot;total_duration&quot;: 1643193699,
  &quot;load_duration&quot;: 13585015,
  &quot;prompt_eval_count&quot;: 8,
  &quot;prompt_eval_duration&quot;: 5000000,
  &quot;eval_count&quot;: 40,
  &quot;eval_duration&quot;: 1623000000
}
</code></pre>
<hr>
<h2 id="简单测试">简单测试</h2>
<ol>
<li>Q:9950处理器性能怎么样？</li>
</ol>
<pre><code>&lt;think&gt;
好，我现在要帮用户分析一下AMD Ryzen 9 5950X处理器的性能。首先，回顾之前的对话历史，用户之前询问了关于显卡部署模型的问题，现在转向CPU性能的讨论。

我需要先了解Ryzen 9 5950X的基本参数：16核32线程，基础频率3.5GHz，加速频率4.8GHz。这些数据能说明它的多任务处理和高负载应用能力很强。

接下来，考虑用户的使用场景可能是什么。他们可能是开发者、研究人员或者需要运行AI模型的人士，因为之前的对话涉及显卡配置和模型部署。所以用户可能关心的是在进行深度学习训练或推理时，这颗CPU的表现如何。

比较对象主要是Intel的i9-10980XE，强调多线程性能较好，但核心数量少于AMD的5950X。这意味着在需要多核的任务上，Ryzen更有优势。

另外，考虑兼容性和支持性：5950X是AM4接口，兼容大多数主板和内存条，这对于用户来说可能是一个优点，因为他们不需要更换平台。

总结一下，我应该强调Ryzen 9 5950X在多核任务上的优势，适合深度学习、视频编辑和游戏等场景，并建议如果有更高需求的话，可以考虑线程撕裂者或霄龙处理器。最后，给出购买建议时，要提到性价比高以及兼容性好。
&lt;/think&gt;

AMD Ryzen 9 5950X 是一款非常强大的高性能 CPU，属于 AMD 的 Ryzen 5000 系列（Zen 3 架构）。以下是对其性能的详细分析：

---

### **主要规格：**
- **核心与线程数**：16 核（8 个物理核心，16 个逻辑核心，超线程技术支持）。
- **基础频率**：3.5 GHz。
- **加速频率**：4.8 GHz（最大 Boost 频率）。
- **缓存**：28 MB 三级缓存。
- **内存支持**：DDR4-3600（最高支持 3600 MHz，ECC 内存支持）。
- **接口**：AM4 接口（与大多数 AM4 主板兼容）。

---

### **性能特点：**
2. **多线程性能强**：
   - Ryzen 9 5950X 是一款非常适合多线程任务的处理器，例如视频编辑、3D 渲染、虚拟机管理、以及深度学习等。
   - 在处理复杂任务时，其 16 核和 32 线程可以显著提高效率。

3. **游戏性能优秀**：
   - 对于游戏来说，Ryzen 9 5950X 的单核性能也非常强大，在大多数游戏中表现接近或超过 Intel 的旗舰处理器（如 i9-10980XE）。
   - 在高刷新率和多任务处理中表现尤为突出。

4. **能效比高**：
   - Zen 3 架构在能效方面有了显著提升，Ryzen 9 5950X 在高性能状态下功耗控制较好，适合长时间运行高强度任务。

5. **兼容性好**：
   - 支持大多数 AM4 主板（包括 B550、B650 等），支持 PCIe 4.0 和 USB 3.2 Gen 1。

---

### **与竞争对手的对比：**
- **Intel i9-10980XE**：
  - Intel 的旗舰处理器在单核性能上略有优势，但在多线程任务中不如 Ryzen 9 5950X。
- **Ryzen 线程撕裂者（Threadripper）**：
  - 如果需要更多核心数（如 24 核或 32 核），可以考虑 Threadripper 系列。

---

### **适合的场景：**
6. **开发与研究**：
   - 适合运行深度学习、AI 推理等任务，尤其是在多线程环境下。
7. **内容创作**：
   - 对于视频编辑、3D 渲染、虚拟现实等任务表现非常出色。
8. **游戏**：
   - 在现代游戏中提供 excellent 性能，尤其在 1080p 或 1440p 分辨率下表现优异。

---

### **总结：**
Ryzen 9 5950X 是一款性价比极高的高性能 CPU，适合需要强大计算能力的用户。对于大多数日常使用场景、游戏和轻度工作站任务来说，它都能提供 excellent 性能体验。如果您需要更高的核心数或更好的单核性能，可以考虑更高端的 Ryzen 线程撕裂者（Threadripper）或霄龙处理器（如霄龙 7000 系列）    
。
</code></pre>
<ol start="2">
<li>Q:我问的是最近上市的9950x,而不是5950x</li>
</ol>
<pre><code>&lt;think&gt;
好的，用户提到他们询问的是最近发布的“9950X”，而之前的回答是关于Ryzen 9 5950X的。这可能是因为用户混淆了型号或者输入错误。

首先，我需要确认是否有名为“9950X”的处理器存在。根据我的知识库，AMD最近发布的产品中没有这个型号。可能是用户记错了型号，比如Ryzen 9 7900X或其他型号。

接下来，我应该推测用户可能是指的其他产品，比如Ryzen 9 7900X或更高端的7950X。如果是这样的话，我可以提供这些处理器的信息来帮助用户。

此外，考虑到用户可能是想了解最新的高性能CPU，我应该向他们介绍最近发布的Ryzen 7000系列，特别是Ryzen 9 7900X和7950X，以及它们的性能特点和适合的应用场景。

最后，我要礼貌地指出用户的可能错误，并提供相关的正确信息，以确保用户能够得到他们需要的帮助。
&lt;/think&gt;

您好！AMD 最近并没有发布一款名为“Ryzen 9 9950X”的处理器。目前 AMD 的最新旗舰处理器是 **Ryzen 9 7900X** 和 **Ryzen 9 7950X**，属于 Ryzen 7000 系列（Zen 4 架构），于 2023 年发布。

如果您是对最近发布的高性能处理器感兴趣，请参考以下信息：

---

### **AMD Ryzen 9 7900X 和 7950X**
- **核心与线程数**：
  - **Ryzen 9 7900X**：12 核（6 物理核心，12 逻辑核心），24 线程。
  - **Ryzen 9 7950X**：16 核（8 物理核心，16 逻辑核心），32 线程。

- **频率**：
  - Base Clock：3.0 GHz 或更高（取决于版本）。
  - Boost Clock：最高可达 5.4 GHz（Ryzen 9 7950X）。

- **缓存**：
  - 三级缓存：64 MB。

- **内存支持**：
  - DDR5 4800 MHz（原生支持，可超频至更高频率）。

- **接口**：
  - LGA 1700（兼容最新的 AM5 主板）。

---

### **性能特点：**
3. **单核与多核性能提升**：
   - Zen 4 架构带来了显著的 IPC（每时钟周期指令数）提升，适合单线程和多线程任务。
4. **游戏性能**：
   - 在最新游戏中表现出色，尤其在 4K 分辨率下表现优异。
5. **能效比高**：
   - 新架构进一步优化了能效，适合长时间运行高强度任务。

---

### **适用场景：**
- 游戏开发、内容创作（如视频编辑、3D 建模）、深度学习与 AI 推理等高性能需求场景。

如果您有更多具体问题，请随时告诉我！

</code></pre>
<p>通过前两个问题发现，deepseek-r1的本地模型知识库并不是最新的，再次提问验证。<br>
3. Q:你的知识库是什么时候更新的？</p>
<pre><code>&lt;think&gt;
好的，用户问及我的知识截止日期是何时。我需要明确告知用户，我的训练数据包括了截至2023年12月的信息，并且我会持续通过互联网获取新知识。

同时，我要礼貌地邀请用户提供更具体的问题，以便我能更好地帮助他们。
&lt;/think&gt;

您好！我的知识库包含了大量的通用信息和最新内容，但具体的更新频率取决于我所使用的数据源。目前，我的训练数据包括了截至 **2023年12月** 的信息。

如果您有任何具体问题或需要最新的信息，请随时告诉我，我会尽力为您提供准确的答案！
</code></pre>
<h2 id="总结">总结</h2>
<p>在多次测试中发现，DeepSeek-R1-14B模型的使用体验远超过llama-13b模型，非常惊喜。但是在使用过程中，仍然需要注意DeepSeek-R1的本地模型知识库更新时间的问题。如果基于该本地模型进行二次开发，比如使用langchain框架赋予本地大模型联网搜索和私有数据库的能力，应该会让使用体验更好。</p>
]]></content>
    </entry>
</feed>